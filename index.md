# NeRF homework implementation

## Introduction
A groundbreaking computer vision concept that emerged in 2020 and has redefined our perception of virtual environments is Neural Radiance Fields (NeRF). The essence of NeRF is the ability to synthesize realistic scenes by capturing the interactions of light and matter. 

This blog aims to introduce the theory behind NeRF and show a code implementation showing how a scene is modeled and rendered, how the model is trained and used as well as challenges encountered. The .ipynb was developed as homework for the computational imaging course and will be used as a base for implementing NeRF.

I hope to provide the first steps to understand the potential of NeRF and to encourage the readers to implement their own versions and extend the functionalities of this notebook.

More details can be found in the original [NeRF webpage](https://www.matthewtancik.com/nerf).

## What is NeRF?

Neural Rendering employs neural networks to simulate the interaction of light and surfaces in a 3D scene. NeRF directly learns the volumetric representation of the scene. It treats a scene as a continuous field of radiance and captures variations in color, lighting and geometry at each point in space. This allows NeRF to capture complex lighting effects, reflections and translucent objects from different angles and viewing positions. [1]

![nerf_theory](https://github.com/TechT3o/NeRF_hw_implementation/assets/87833804/d1bf35b6-b7fa-46b6-9951-a7faaa491c17)

*Figure 1:  NeRF  view synthesis diagram*



### Scene representation

The first step in implementing NeRF is to be able to represent the 3D scene. We define functions that initialize the ray directions as commonly done in Ray Tracing applications. As well as a function that used the camera fundamental matrix to transform the rays of each camera / viewpoint to world coordinates. Using these ray directions we can create a grid of the query points that represent the scene points where we will use the neural network to find the color and opacity required to do volumetric rendering.

These functions are used in both the training process and later when evaluating the model.

### Scene Rendering

By inputting these coordinate points to the neural network the opacities and the RGB color value at each of these points is found. If a point is very opaque it will affect the ray’s color a lot while if it is clear it will not. These opacities are used to obtain weights for how much each point will affect the rendered image for a given ray. Then we do volume rendering by summing the weighted color values along each ray and obtain the 2D rendered RGB image of a particular viewpoint

### Data

The data used are images of a lego bulldozer taken from multiple poses and focal lengths. In order to help the model learn high frequency representations a positional encoding method that extracts Fourier feature maps was used. More details about this encoding can be found [here](https://bmild.github.io/fourfeat/index.html). [2]

### Model

The model used is a multi layer perceptron with 8 256 neuron intermediate layers and skip connections every 4 layers. The input size is equal to 3+6*the size of the positional encoding and the output has size of 4, 3 for the rgb color value and 1 for the opacity. 

### Training

For training the model is trained for 500 iterations, with batch size of 64 samples and the PSNR metric is monitored until it reaches a value of 22. 

![image2](https://github.com/TechT3o/NeRF_hw_implementation/assets/87833804/8b672279-f08a-4f40-9aaf-d986d422b15c)


*Figure 2: 360 view generated by this NeRF implementation*


## Why is NeRF Important?

NeRF with its ability to synthesize realistic scenes, obtain depth maps of even translucent objects and dynamically adjust the lighting of a scene while using a neural network that does not take too much memory space made it appealing for applications such as:  

### Virtual Reality applications

NeRF could be used to reconstruct real world scenes where every detail in the room is reconstructed from the texture of the objects to the reflections of shiny surfaces.

### Visualizations

Accurate and realistic representations of product prototypes, architectural designs etc.

### 3D Scene storage method

Promising method to store 3D assets in low sized files. 

## NeRF extensions
In order to address some of NeRFs problems such as long rendering times, long training times, requiring many photographs around the scene and aliasing when images of different resolutions and distances are given researchers have created extensions such as:

### [NeRV (Neural Reflectance and Visibility Fields)](https://pratulsrinivasan.github.io/nerv/)

NeRV incorporates reflectance and visibility improving handling transparent and reflective surfaces. [3]

### [PlenOctrees](https://alexyu.net/plenoctrees/)

Plenoctrees fuse neural representations with octrees achieving robustness and a much faster rendering time. [4]

### [NeRF in the wild](https://nerf-w.github.io/)

NeRF in the wild optimizes NeRF to be able to work well in real-world scenes with dynamic elements and diverse lighting conditions. [5]

### [pixelNeRF](https://alexyu.net/pixelnerf/)

pixelNeRF employs pixel-aligned feature vectors that allow it to capture fine details in scenes. [6]

### [Mega-NeRF](https://meganerf.cmusatyalab.org/)

Mega-NeRF expands NeRF to work with larger-scale scenes. [7]

### [LOLNeRF (Learning Optical Lightness for NeRF)](https://ubc-vision.github.io/lolnerf/)

LOLNeRF explores techniques that improve the model’s performance in diverse lighting. [8]

### [Mip-NeRF](https://jonbarron.info/mipnerf/)

Mip-NeRF introduces a mipmapping strategy to make NeRF more robust to images of different resolution and distances from objects and faster rendering times. [9]

### [KiloNeRF](https://github.com/creiser/kilonerf)

KiloNeRF  replaces the MLP with thousands of tiny MLPs that accelerate training and rendering with minimal sacrifice to visual quality. [10]

## Conclusion
Concluding this introduction, we have explored the workings and power of NeRF in synthesizing realistic 3D scenes with intricate lighting effects and lifelike textures transitioning from traditional geometric models to volumetric scene representations.

Extensions of this technique that improve the original NeRF in terms of scalability, efficiency, handling dynamic scenes and overall robustness are mentioned and a simple Python implementation using Pytorch is provided.
Explore the NeRF notebook.
The .ipynb with the code of this project is found in [11]: [https://colab.research.google.com/drive/1G4mVQJiYDXeubstmtwNTQ9wS2wRi_kjF?usp=sharing](https://colab.research.google.com/drive/1G4mVQJiYDXeubstmtwNTQ9wS2wRi_kjF?usp=sharing) .

Feel free to experiment with this code and feel free to contact me to show me your work or discuss.



## References
[1] Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., & Ng, R. (2021). Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1), 99-106.

[2] Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., ... & Ng, R. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems, 33, 7537-7547.

[3] Srinivasan, P. P., Deng, B., Zhang, X., Tancik, M., Mildenhall, B., & Barron, J. T. (2021). Nerv: Neural reflectance and visibility fields for relighting and view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7495-7504).

[4] Yu, A., Li, R., Tancik, M., Li, H., Ng, R., & Kanazawa, A. (2021). Plenoctrees for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 5752-5761).

[5] Martin-Brualla, R., Radwan, N., Sajjadi, M. S., Barron, J. T., Dosovitskiy, A., & Duckworth, D. (2021). Nerf in the wild: Neural radiance fields for unconstrained photo collections. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7210-7219).

[6] Yu, A., Ye, V., Tancik, M., & Kanazawa, A. (2021). pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4578-4587).

[7] Turki, H., Ramanan, D., & Satyanarayanan, M. (2022). Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12922-12931).

[8] Rebain, D., Matthews, M., Yi, K. M., Lagun, D., & Tagliasacchi, A. (2022). Lolnerf: Learn from one look. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1558-1567).

[9] Barron, J. T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., & Srinivasan, P. P. (2021). Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 5855-5864).

[10] Reiser, C., Peng, S., Liao, Y., & Geiger, A. (2021). Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 14335-14345).

[11] Simple NeRF Torch implementation .ipynb: [https://colab.research.google.com/drive/1G4mVQJiYDXeubstmtwNTQ9wS2wRi_kjF?usp=sharing](https://colab.research.google.com/drive/1G4mVQJiYDXeubstmtwNTQ9wS2wRi_kjF?usp=sharing)
